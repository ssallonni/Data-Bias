# Data-Bias
This is my public Git Repository for I310D Coding Assignment 3

After reading about how the Perspective API can measure how offesnive a comment is in multiple langauages, **my hypothesis for this porject was to compare weather the API is able to accurately measure toxic and non-toxic comments Hindi (Hinglish) equally as well as it would be able to when translated in English.** I feel that it's pretty safe to assume that the Perspective API can more accurately score toxic and non-toxic statements written in English whihch I why I believe that the Perspective API would be able to more accurately score statements in English than in in Hindi (Hinglish).

To test my hypothesis, I came up with 60 statemnts in the following four categories: 15 non-toxic in Hinglish, 15 toxic in English, 15 non-toxic in English, and 15 toxic in English. I came up with the statements in Hinglish originally and then translated them to English. The results that I got showed that the perspective API was able to accurately score ___ of the statements for the non-toxic Hinglish, non-toxic English, and toxic English. For the toxic Hinglish statements, the perspective API was only able to accurately score ___%. The score for the three incorrect scores were 0.___, 0.___, and 0.___. Although I'm not suprised that they were incorrectly calculated 

I feel like the reason for this is because certain words in the Hindi language crossover with other languages that change its meaning. Before finalizing my list of toxic Hinglish statements, when I would test them in the Perspective API, it was unable to give me a proper score because the language detected was not a part of their list of availible langauges. Other times, multiple languages including Hinglish were detected whihc could have affected the outputted score. I was ultimately looking for statements that could only detected Hinglish and no other language.

I feel that because the models is trained on predominantly English data, it might not perform as well for languages with significantly different linguistic structures. Other biases include how toxicity and offensiveness can be highly context-dependent and culturally specific. Phrases or expressions considered offensive in one culture might not be offensive in another. The model's training data, which is likely biased towards certain cultural contexts, might not accurately capture the nuances of all cultures and languages.
