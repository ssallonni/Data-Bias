{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssallonni/Data-Bias/blob/main/Saloni_Sharma_Coding_Assignment_Data_Bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only load the test dataset Bias-Test-Data.csv."
      ],
      "metadata": {
        "id": "CcesbWCVW6eq"
      },
      "id": "CcesbWCVW6eq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27236d66",
      "metadata": {
        "scrolled": false,
        "id": "27236d66"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"Bias-Test-Data.csv\")\n",
        "test_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are calling the perspective API from Google Cloud as well as creating a new column in our datafram that holds all the calculated scores"
      ],
      "metadata": {
        "id": "wG7Md9FRXE2u"
      },
      "id": "wG7Md9FRXE2u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01514e6",
      "metadata": {
        "scrolled": true,
        "id": "b01514e6"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyAoAnnNH5d6lSYGuMs0tZZqXksnpF2enUM'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "scoresList = []\n",
        "comment_column = test_df['Comment']\n",
        "for comment in comment_column:\n",
        "  analyze_request = {\n",
        "    'comment': { 'text': comment },\n",
        "    'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "\n",
        "  response = client.comments().analyze(body=analyze_request).execute()\n",
        "  step1 = response['attributeScores']\n",
        "  step2 = step1['TOXICITY']\n",
        "  step3 = step2['spanScores'][0]\n",
        "  step4 = step3[\"score\"]\n",
        "  step5 = step4['value']\n",
        "  scoresList.append(step5)\n",
        "  #print(comment)\n",
        "  #print(step5)\n",
        "test_df['Score'] = scoresList\n",
        "print(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating a new column in our dataframe tha holds all the predictions based on the score"
      ],
      "metadata": {
        "id": "jZn0bKy-WpAa"
      },
      "id": "jZn0bKy-WpAa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b03932c",
      "metadata": {
        "id": "1b03932c"
      },
      "outputs": [],
      "source": [
        "\n",
        "thresholdValue = 0.5\n",
        "predictions = []\n",
        "score_column = test_df['Score']\n",
        "for score in score_column:\n",
        "    if score >= thresholdValue:\n",
        "        prediction = 'Toxic'\n",
        "    else:\n",
        "        prediction = 'Non-toxic'\n",
        "    predictions.append(prediction)\n",
        "\n",
        "test_df['Prediction'] = predictions\n",
        "print(test_df)\n",
        "\n",
        "accuracy_column = test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing Model Fairness w.r.t Language\n"
      ],
      "metadata": {
        "id": "sb9x-Yd_WU1r"
      },
      "id": "sb9x-Yd_WU1r"
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = test_df['Type']\n",
        "y_predicted = test_df['Prediction']\n",
        "\n",
        "language_column = test_df['Language']\n",
        "\n",
        "hinglish_indices = []\n",
        "english_indices = []\n",
        "\n",
        "for i in range(len(language_column)):\n",
        "    if language_column[i] == 'Hinglish':\n",
        "        hinglish_indices.append(i)\n",
        "    else:\n",
        "        english_indices.append(i)\n",
        "\n",
        "y_actual_hinglish = [y_actual[i] for i in hinglish_indices]\n",
        "y_predicted_hinglish = [y_predicted[i] for i in hinglish_indices]\n",
        "\n",
        "y_actual_english = [y_actual[i] for i in english_indices]\n",
        "y_predicted_english = [y_predicted[i] for i in english_indices]\n",
        "\n",
        "print (len(hinglish_indices))\n",
        "print (len(english_indices))"
      ],
      "metadata": {
        "id": "W4vTi_kdSHnv"
      },
      "id": "W4vTi_kdSHnv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute class wise accuracy for Hinglish and English categories"
      ],
      "metadata": {
        "id": "vjUrnbx2XTHs"
      },
      "id": "vjUrnbx2XTHs"
    },
    {
      "cell_type": "code",
      "source": [
        "def class_wise_acc(y_actual, y_predicted):\n",
        "    total_p = 0\n",
        "    total_n = 0\n",
        "    TP=0\n",
        "    TN=0\n",
        "    for i in range(len(y_predicted)):\n",
        "        if y_actual[i] == 'Toxic':\n",
        "            total_p = total_p+1\n",
        "            if y_actual[i]==y_predicted[i]:\n",
        "               TP=TP+1\n",
        "        if y_actual[i] == 'Non-toxic':\n",
        "            total_n=total_n+1\n",
        "            if y_actual[i]==y_predicted[i]:\n",
        "               TN=TN+1\n",
        "    return(TP/total_p, TN/total_n)\n",
        "\n",
        "class_1_acc_hinglish, class_0_acc_hinglish = class_wise_acc(y_actual_hinglish, y_predicted_hinglish)\n",
        "class_1_acc_english, class_0_acc_english = class_wise_acc(y_actual_english, y_predicted_english)\n",
        "\n",
        "print (f\"Class Non-Toxic (i.e., <0.5) accuracy for Hinglish = {class_0_acc_hinglish}\")\n",
        "print (f\"Class Toxic (i.e., >0.5) accuracy for Hinglish = {class_1_acc_hinglish}\")\n",
        "print (f\"Class Non-Toxic (i.e., <0.5) accuracy for English = {class_0_acc_english}\")\n",
        "print (f\"Class Toxic (i.e., >0.5) accuracy for English = {class_1_acc_english}\")\n"
      ],
      "metadata": {
        "id": "T_mtw_lkSJOC"
      },
      "id": "T_mtw_lkSJOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All insights are provided in the README file in my GitHub repository"
      ],
      "metadata": {
        "id": "F6_wzB41Xo23"
      },
      "id": "F6_wzB41Xo23"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}